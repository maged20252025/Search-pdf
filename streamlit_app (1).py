
import streamlit as st
import fitz  # PyMuPDF
import io
import zipfile
import re
from collections import defaultdict

st.set_page_config(page_title="Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„ÙØ§Øª PDF", layout="wide")
st.title("ğŸ“„ Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ù…Ù„ÙØ§Øª PDF (Ù†ØµÙŠØ© ÙÙ‚Ø·)")

# ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
def normalize_text(text):
    text = re.sub(r"[Ù‘ÙÙ‹ÙÙŒÙÙÙ’Ù€]", "", text)
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ø©", "Ù‡")
    return text.strip()

# ØªÙ…ÙŠÙŠØ² Ø§Ù„ÙƒÙ„Ù…Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ
def highlight_word(text, word):
    pattern = re.compile(re.escape(word), re.IGNORECASE)
    return pattern.sub(lambda m: f"<mark style='background-color: #fff176'>{m.group(0)}</mark>", text)

uploaded_files = st.file_uploader("ğŸ“¤ Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª PDF ÙÙ‚Ø· (Ø­ØªÙ‰ 50 Ù…Ù„Ù)", type=["pdf"], accept_multiple_files=True)

keywords_input = st.text_area("âœï¸ Ø£Ø¯Ø®Ù„ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« (Ø§ÙØµÙ„ Ø¨ÙŠÙ†Ù‡Ø§ Ø¨ÙØ§ØµÙ„Ø©)", "")
search_btn = st.button("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø«")

if uploaded_files and search_btn and keywords_input.strip():
    raw_keywords = [k.strip() for k in keywords_input.split(",") if k.strip()]
    normalized_keywords = [normalize_text(k) for k in raw_keywords]

    results = []
    matched_files = {}
    word_counts = defaultdict(int)
    skipped_files = []

    for uploaded_file in uploaded_files:
        file_name = uploaded_file.name
        file_bytes = uploaded_file.read()
        uploaded_file.seek(0)

        try:
            pdf = fitz.open(stream=io.BytesIO(file_bytes), filetype="pdf")
        except:
            st.error(f"âŒ ØªØ¹Ø°Ø± ÙØªØ­ Ø§Ù„Ù…Ù„Ù: {file_name}")
            continue

        found_text = False

        for page in pdf:
            text = page.get_text()
            if not text.strip():
                continue

            found_text = True
            lines = [line.strip() for line in text.split("\n") if line.strip()]

            for line in lines:
                norm_line = normalize_text(line)
                for raw_kw, norm_kw in zip(raw_keywords, normalized_keywords):
                    if norm_kw in norm_line:
                        word_counts[raw_kw] += 1
                        highlighted = highlight_word(line, raw_kw)
                        results.append({"file": file_name, "text": highlighted})
                        matched_files[file_name] = file_bytes

        if not found_text:
            skipped_files.append(file_name)

    if results:
        st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø© ÙÙŠ {len(matched_files)} Ù…Ù„Ù.")
        st.markdown("---")

        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª:")
        for word, count in word_counts.items():
            st.write(f"ğŸ”¹ '{word}': {count} Ù…Ø±Ø©")

        st.markdown("---")
        st.subheader("ğŸ“„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
        for res in results:
            st.write(f"ğŸ“˜ **Ø§Ù„Ù…Ù„Ù:** {res['file']}")
            st.markdown(res["text"], unsafe_allow_html=True)

        st.markdown("---")
        st.subheader("â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ Ø¸Ù‡Ø±Øª ÙÙŠÙ‡Ø§ Ù†ØªØ§Ø¦Ø¬:")

        for name, content in matched_files.items():
            st.download_button(f"ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {name}", data=content, file_name=name)

        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, "w") as zipf:
            for filename, content in matched_files.items():
                zipf.writestr(filename, content)
        zip_buffer.seek(0)

        st.download_button("ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© (ZIP)", data=zip_buffer, file_name="matched_pdfs.zip")
    else:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù†ØªØ§Ø¦Ø¬.")

    if skipped_files:
        st.info("â„¹ï¸ ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ø£Ù†Ù‡Ø§ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ù†ØµÙ‹Ø§ Ù‚Ø§Ø¨Ù„Ø§Ù‹ Ù„Ù„Ø¨Ø­Ø«:")
        for name in skipped_files:
            st.write(f"ğŸ”¸ {name} - Ù…Ù„Ù Ù…Ù…Ø³ÙˆØ­ Ø¶ÙˆØ¦ÙŠÙ‹Ø§ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ OCR ØºÙŠØ± Ù…ÙØ¹Ù„ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¥ØµØ¯Ø§Ø±.")
